{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnb Seattle Reservation Prices Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Insights\n",
    "Before diving deep into data, let's find out some general information on how the reservation price is determined and search for some useful facts for the future analysis.\n",
    "\n",
    "AirBnB reservation price is based on following costs (according to AirBnB official website information):\n",
    "   - Costs determined by the host:\n",
    "       - Nightly price: Nightly rate decided by the host\n",
    "       - Cleaning fee: One-time fee charged by some hosts to cover the cost of cleaning their space\n",
    "       - Extra guest fees: One-time fee charged by some hosts to cover other costs related to using their space\n",
    "   - Costs determined by Airbnb: Airbnb service fee\n",
    "   - Other costs that may be included\n",
    "       - currency exchange fees\n",
    "       - VAT\n",
    "       - other local taxes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis, we will concentrate on the factors, which influence the nightly price, determined by the host. Actually, AirBnB already has an algorithm, which suggests hosts the price. The approach for the algorithm and challenges are described in the article here. We can also use the information from the article in our analysis:\n",
    "   - Time of the year, when reservation is made, affects the price\n",
    "   - Amenities offered like Wi-Fi and TV should be considered in the analysis, as they also might have great influence on price\n",
    "   - It is better to use neighbourhood information rather than exact coordinates to describe the listing location, because sometimes even close locations might have huge difference in listing prices, if they belong to different neighbourhoods or are located differnet sides of the river"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "First of all, I chose Seattle dataset, and it will take me the following steps to come into conclusions about factors, affecting reservation prices:\n",
    "   1. The first look at the data\n",
    "       - what information we have?\n",
    "       - what information is missing?\n",
    "       - discover general facts: time period, total number of listings in the dataset\n",
    "   2. Initial data preparation\n",
    "       - remove the irrelevant information\n",
    "       - reformat the information and imputing missing values\n",
    "   3. High level analysis\n",
    "       - find out high level trends and correlations\n",
    "   4. Use machine learning for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = pd.read_csv('listings.csv')\n",
    "\n",
    "listings_number = listings_df.shape[0]\n",
    "hosts_number= len(listings_df['host_id'].unique())\n",
    "\n",
    "print('listings.csv dataset contains information on %d listings provided by %d hosts.'%(listings_number, hosts_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of missing values for each column\n",
    "listings_missing_df = listings_df.isnull().mean()*100\n",
    "\n",
    "#filter out only cols, which have missing values\n",
    "listings_cols_with_nan = listings_missing_df[listings_missing_df > 0]\n",
    "\n",
    "#plot the result\n",
    "listings_cols_with_nan.plot.bar(title = 'Missing Values per Column, %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of rows in dataset\n",
    "rows_num = calendar_df.shape[0]\n",
    "\n",
    "# find first and last date\n",
    "min_date = calendar_df['date'].min()\n",
    "max_date = calendar_df['date'].max()\n",
    "\n",
    "print('calendar.csv dataset contains %d rows.' % (rows_num))\n",
    "print('The first date is %s, and the last date is %s'%(min_date, max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values\n",
    "calendar_missing_df = pd.DataFrame([calendar_df.isnull().mean()*100])\n",
    "\n",
    "# plot\n",
    "calendar_missing_df.plot.bar(title = 'Missing Values per Column, %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to get the first outcomes, the data should be pre-processed:\n",
    "   - datasets should be merged into one using the listing identifier\n",
    "   - irrelevant columns and columns, which contain missing data should be removed from the analysis\n",
    "   - dates and prices should be converted from text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for dataset cleaning\n",
    "\n",
    "def get_month_from_date(row):\n",
    "    '''\n",
    "    Get month from date represented as a string\n",
    "    '''\n",
    "    return int(row['date'].split('-')[1])\n",
    "\n",
    "def get_year_from_date(row):\n",
    "    '''\n",
    "    Get year from date represented as a string\n",
    "    '''\n",
    "    return int(row['date'].split('-')[0])\n",
    "\n",
    "def get_host_since_year(row):\n",
    "    '''\n",
    "    Get year from a date represented as a string\n",
    "    '''\n",
    "    try:\n",
    "        host_since = int(row['host_since'].split('-')[0])\n",
    "    except:\n",
    "        host_since = np.nan\n",
    "    return host_since\n",
    "\n",
    "def get_val_from_list(row, column_name, value):\n",
    "    '''\n",
    "    Fill in dummy column for values\n",
    "    '''\n",
    "    val = 0.0\n",
    "    try:\n",
    "        vals = row[column_name].replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '').replace('{', '').replace('}', '').split(',')\n",
    "        \n",
    "        if value in vals:\n",
    "            val = 1.0\n",
    "    except:\n",
    "        val = 0.0\n",
    "    \n",
    "    return val\n",
    "\n",
    "def split_list_into_columns(df, column_name, max_dummies_num=10):\n",
    "    '''\n",
    "    Split values in columns, which contain lists (e.g. amenities)\n",
    "    '''\n",
    "    # get dictionary of unique values in lists across dataset rows\n",
    "    value_dict = {}\n",
    "    \n",
    "    for unique_value in df[column_name].unique():\n",
    "        uv = unique_value.replace('[', '').replace(']', '').replace(\"'\", \"\").replace('\"', '').replace('{', '').replace('}', '').split(',')\n",
    "\n",
    "        for value in uv:\n",
    "            if value in value_dict:\n",
    "                value_dict[value] += 1\n",
    "            else:\n",
    "                value_dict[value] = 0\n",
    "\n",
    "    values_sorted = sorted(value_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # split into columns\n",
    "    for value in values_sorted[:max_dummies_num]:\n",
    "        df[column_name + '_' + value[0]] = df.apply(lambda row: get_val_from_list(row, column_name, value[0]), axis=1)\n",
    "    \n",
    "    return \n",
    "\n",
    "def get_extra_people_fee(row):\n",
    "    '''\n",
    "    Return 1 when there is a fee for extra people\n",
    "    '''\n",
    "    if row['extra_people'] == '$0.00':\n",
    "        return 0.0\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset cleaning function\n",
    "def clean_dataset(listings_df, calendar_df):\n",
    "    '''\n",
    "    INPUT\n",
    "    listings_df - pandas df containing listings data\n",
    "    calendar_df - pandas df containing calendar data\n",
    "    \n",
    "    OUTPUT\n",
    "    df - cleaned dataset, which contains merged tables:\n",
    "        1. irrelevant cols are dropped\n",
    "        2. string containing dates are converted into numbers\n",
    "        3. cols, containing lists, are split into several columns\n",
    "        4. missing values are imputed with mean or mode\n",
    "    '''\n",
    "    # merge\n",
    "    listings_df = listings_df.rename(index=str, columns={'id': 'listing_id'})\n",
    "    df = pd.merge(calendar_df, listings_df, on = 'listing_id')\n",
    "    \n",
    "    # drop irrelevant cols\n",
    "    columns_to_drop = ['available', 'host_id','host_location','host_acceptance_rate','host_neighbourhood',\n",
    "                   'host_total_listings_count', 'weekly_price', 'monthly_price',\n",
    "                   'security_deposit', 'cleaning_fee', 'calendar_updated',\n",
    "                   'listing_url','last_scraped' ,'scrape_id', 'name', 'summary', 'space', 'description',\n",
    "                   'experiences_offered', 'street', 'neighbourhood', 'neighbourhood_cleansed', 'zipcode',\n",
    "                   'neighborhood_overview', 'notes', 'transit', 'thumbnail_url', 'medium_url', 'picture_url',\n",
    "                   'xl_picture_url', 'host_url', 'host_name', 'host_about', 'host_thumbnail_url', 'host_picture_url',\n",
    "                   'city', 'state', 'market', 'smart_location', 'country_code', 'country', 'latitude', 'longitude',\n",
    "                   'is_location_exact', 'square_feet', 'has_availability', 'availability_30',\n",
    "                   'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped',\n",
    "                  'first_review', 'last_review', 'requires_license', 'license', 'jurisdiction_names', 'price_y',\n",
    "                  'reviews_per_month']\n",
    "    \n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # convert date from calendar into month and drop the original column\n",
    "    df['month'] = df.apply(lambda row: get_month_from_date(row), axis=1)\n",
    "    df['year'] = df.apply(lambda row: get_year_from_date(row), axis=1)\n",
    "    df = df.drop(columns = ['date'])\n",
    "    \n",
    "    # remove rows where price_x == nan, because price will be used as response column\n",
    "    df = df.dropna(subset=['price_x'])\n",
    "    \n",
    "    # convert price to number and drop the price_x column\n",
    "    df['price'] = df['price_x'].astype(str)\n",
    "    df['price'] = df['price'].str.replace('[$, ]', '').astype('float')\n",
    "    df = df.drop(columns = ['price_x'])\n",
    "    \n",
    "    # convert host_since date into number and fill in missing values, drop the original column\n",
    "    df['host_since_year'] = df.apply(lambda row: get_host_since_year(row), axis=1)\n",
    "    df['host_since_year'].fillna(df['host_since_year'].mean(), inplace=True)\n",
    "    df = df.drop(columns=['host_since'])\n",
    "    \n",
    "    # convert host_response_rate into number and fill in missing values, drop the original column\n",
    "    df['host_response_rate_num'] = df['host_response_rate'].astype(str)\n",
    "    df['host_response_rate_num'] = df['host_response_rate_num'].str.replace('%', '').astype('float')\n",
    "    df['host_response_rate_num'].fillna(df['host_response_rate_num'].mean(), inplace = True)\n",
    "    \n",
    "    df['host_response_rate_buckets'] = pd.qcut(df['host_response_rate_num'], 5, labels=False, duplicates = 'drop')\n",
    "    df = df.drop(columns = ['host_response_rate', 'host_response_rate_num'])\n",
    "    \n",
    "    # fill missing values with mean value for host_listings_count\n",
    "    df['host_listings_count'].fillna(df['host_listings_count'].mean(), inplace=True)\n",
    "    \n",
    "    # split host_verifications into dummy columns and drop the original column\n",
    "    split_list_into_columns(df, 'host_verifications')\n",
    "    df = df.drop(columns = ['host_verifications'])\n",
    "    \n",
    "    # fill in missing values for bathrooms, bedrooms and beds with mode\n",
    "    df['bathrooms'] = df['bathrooms'].fillna(df['bathrooms'].mode()[0])\n",
    "    df['bedrooms'] = df['bedrooms'].fillna(df['bedrooms'].mode()[0])\n",
    "    df['beds'] = df['beds'].fillna(df['beds'].mode()[0])\n",
    "    \n",
    "    # split amenities into dummy columns and drop the original column\n",
    "    split_list_into_columns(df, 'amenities')\n",
    "    df = df.drop(columns = ['amenities'])\n",
    "    \n",
    "    # turn extra people fee into binary column\n",
    "    df['extra_people_fee'] = df.apply(lambda row: get_extra_people_fee(row), axis=1)\n",
    "    df = df.drop(columns=['extra_people'])\n",
    "    \n",
    "    # fill missing values for review scores columns\n",
    "    review_scores_columns = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                         'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                        'review_scores_value']\n",
    "    \n",
    "    for column in review_scores_columns:\n",
    "        df[column].fillna(df[column].mean(), inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(listings_df, calendar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing Prices Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min, max and avg price for listing\n",
    "min_price = df['price'].min()\n",
    "max_price = df['price'].max()\n",
    "avg_price = df['price'].mean()\n",
    "\n",
    "print('Minimum price per listing is $%d.'%(min_price))\n",
    "print('Maximum price per listing is $%d.'%(max_price))\n",
    "print('Average price per listing is $%d.'%(avg_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the avg price for each listing\n",
    "mean_price_for_listing = df.groupby('listing_id').mean()['price']\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(mean_price_for_listing, bins=20)\n",
    "plt.xticks(np.arange(0, 1700, step=100))\n",
    "plt.ylabel('Number of Listings')\n",
    "plt.xlabel('Price, $')\n",
    "plt.title('Number of Listings depending on Price')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of listings is between $ 50 - $ 150 per night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the Busiest Times of the Year to Visit Seattle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how the number of listings change depending on time of the year?\n",
    "- how prices change depending on time of time of the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of total number of listings for each month in 2016\n",
    "number_of_listings_by_month = pd.Series([12])\n",
    "\n",
    "for i in range(1, 13):\n",
    "    number_of_listings_by_month[i] = len(df[(df['month']==i) & (df['year'] == 2016)]['listing_id'].unique())\n",
    "\n",
    "number_of_listings_by_month = number_of_listings_by_month.drop(0)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(number_of_listings_by_month)\n",
    "plt.xticks(np.arange(1, 13, step=1))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Listings')\n",
    "plt.title('Number of Listings per Month, 2016')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that number of availabel listings for reservation tends to be the lowest in summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average price by month\n",
    "average_price_by_month = df.groupby(['month'])['price'].mean()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(average_price_by_month)\n",
    "plt.ylabel('Average price, $')\n",
    "plt.xlabel('Month')\n",
    "plt.title('Average Price')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that listing prices raise significantly in summer, because of the fact that there are less listigns available for reservation. There is also a raise in December. This tells us that, summer and winter holidays should be the busiest times to visit Seattle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices Depending on Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of neighbourhoods\n",
    "neighbourhoods = df['neighbourhood_group_cleansed'].unique()\n",
    "\n",
    "# get prices by month and neighbourhood\n",
    "price_by_month_neighbourhood = df.groupby(['month', 'neighbourhood_group_cleansed']).mean().reset_index()\n",
    "\n",
    "# plot prices for each neighbourhood\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for neighbourhood in neighbourhoods:\n",
    "    ax.plot(price_by_month_neighbourhood[price_by_month_neighbourhood['neighbourhood_group_cleansed'] == neighbourhood]['month'], price_by_month_neighbourhood[price_by_month_neighbourhood['neighbourhood_group_cleansed'] == neighbourhood]['price'], label = neighbourhood)\n",
    "    \n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width*0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.ylabel('Average price, $')\n",
    "plt.xlabel('Month')\n",
    "plt.title('Average price for Neighbourhood, $')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest average prices are in Downtown. All neighbourhoods' prices tend to raise in summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Price and Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'host_since_year', 'host_listings_count', 'extra_people_fee', 'review_scores_rating', 'price']\n",
    "\n",
    "corrs = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale = 1)\n",
    "sns.set(rc={'figure.figsize':(7, 7)})\n",
    "hm = sns.heatmap(corrs, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols, xticklabels=cols).set_title('Correlation Map')\n",
    "\n",
    "fig = hm.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that price is correlated with number of accomodates, bathrooms, bedrooms and beds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two popular models: Random Forest Regressor, XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn categorical columns into dummies\n",
    "cat_columns = list(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in cat_columns:\n",
    "    df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=True)], axis=1)\n",
    "\n",
    "df = df.drop(columns = ['listing_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepair train, test dataset for modelling\n",
    "TEST_SIZE = 0.3\n",
    "RAND_STATE = 42\n",
    "\n",
    "X = df.drop(columns='price')\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, random_state = RAND_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, criterion='mse', random_state = RAND_STATE, n_jobs=1)\n",
    "\n",
    "# df.squeeze(): squeeze 1 dimensional axis objects into scalars\n",
    "rf.fit(X_train, y_train.squeeze())\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "print('Random Forest MSE train: %.3f, test: %.3f' %(mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))\n",
    "print('Random Forest R^2 train: %.3f, test: %.3f'%(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances from the model\n",
    "headers = ['name', 'score']\n",
    "\n",
    "values = sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1]*-1)\n",
    "\n",
    "forest_feature_importances = pd.DataFrame(values, columns=headers)\n",
    "forest_feature_importances = forest_feature_importances.sort_values(by=['score'], ascending=False)\n",
    "\n",
    "features = forest_feature_importances['name'][:15]\n",
    "y_pos = np.arange(len(features))\n",
    "scores = forest_feature_importances['score'][:15]\n",
    "\n",
    "# plot feature importances\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, features, rotation='vertical')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Features')\n",
    "plt.title('Feature importances (Random Forest)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一用xgboost.fit就会崩溃 不知道为什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f'%(mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f'%(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
